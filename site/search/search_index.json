{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to BytePrep \u2013 Your Data Engineering Interview Hub \ud83d\ude80","text":"<p>BytePrep is a curated, no-fluff guide designed to help you prepare for Data Engineering interviews. Whether you're brushing up on fundamentals or tackling real-world scenarios, this platform has you covered across:</p> <ul> <li>\u2705 SQL and query optimization</li> <li>\u2705 Python for data engineering</li> <li>\u2705 PySpark and distributed processing</li> <li>\u2705 AWS and cloud-native services</li> <li>\u2705 System design for data-intensive systems</li> <li>\u2705 Behavioral and scenario-based questions</li> <li>\u2705 End-to-end data engineering project walkthroughs</li> </ul> <p>\ud83d\udca1 This is a living resource. Stay consistent, practice regularly, and level up your skills to land your dream DE role!</p> <p>Use the navigation menu to dive into specific topics and start your preparation journey. \u2728</p>","tags":["data-engineering","interview-prep","sql","python","pyspark","aws","system-design","cloud","big-data","data-pipelines"]},{"location":"about/","title":"About BytePrep","text":""},{"location":"about/#about-byteprep","title":"About BytePrep","text":"<p>BytePrep is a curated, no-fluff resource built by Anand Kumar Singh to help aspiring and experienced data engineers prepare for technical interviews with confidence.</p> <p>This guide brings together real-world, scenario-based questions across essential domains like SQL, Python, PySpark, AWS, system design, data modeling, and more.</p> <p>Built using MkDocs and the Material for MkDocs theme, the site is designed to be fast, searchable, and content-rich \u2014 without distractions.</p> <p>\ud83d\udccc This is an evolving resource. New questions, patterns, and project examples will be continuously added. Stay consistent, and good luck on your data engineering journey!</p>"},{"location":"aws/","title":"AWS","text":"<p>title: AWS Interview Questions description: Explore real-world AWS scenario-based questions covering Glue, S3, Lambda, and best practices for data engineering interviews.</p>"},{"location":"aws/#aws-scenario-based-questions","title":"AWS Scenario-Based Questions","text":""},{"location":"aws/#1-aws-glue-scenario","title":"1. AWS Glue Scenario","text":"<p>Question: You have a daily batch job that ingests CSV files from an S3 bucket, performs transformations, and writes to a Snowflake data warehouse. Occasionally, the schema of incoming files changes (e.g., new columns are added). How do you handle schema evolution in AWS Glue?</p> <p>Answer: - Use AWS Glue DynamicFrame with <code>mergeSchema=True</code> to handle schema evolution. - Create a Glue Crawler that runs before the ETL job to update the Data Catalog. - In PySpark code, dynamically reference columns using <code>df.columns</code> and use conditional logic (<code>if 'new_col' in df.columns</code>) for optional fields. - Update the Snowflake destination table with new columns and use <code>MERGE</code> logic to maintain idempotency.</p>"},{"location":"aws/#2-amazon-s3-scenario","title":"2. Amazon S3 Scenario","text":"<p>Question: You are storing large amounts of data in S3 that are accessed infrequently but must be retrieved within a few minutes when needed. You also want to reduce costs. Which storage class should you use and how will you automate the transition?</p> <p>Answer: - Use S3 Intelligent-Tiering or S3 Standard-IA (Infrequent Access) based on access patterns. - Set up an S3 Lifecycle policy to transition objects to the desired storage class after a specific number of days (e.g., 30 days). - Enable object-level logging and monitoring via CloudWatch to refine lifecycle rules over time.</p>"},{"location":"aws/#3-aws-lambda-scenario","title":"3. AWS Lambda Scenario","text":"<p>Question: You have an S3 bucket that receives real-time uploads of images. You want to resize and store them in another bucket as thumbnails. How do you implement this using AWS Lambda?</p> <p>Answer: - Configure the source S3 bucket to trigger a Lambda function on <code>s3:ObjectCreated</code> events. - The Lambda function (using Python or Node.js) reads the image using <code>boto3</code>, processes it using <code>Pillow</code> or <code>Sharp</code>, and writes the thumbnail to a destination S3 bucket. - Ensure the Lambda has appropriate IAM permissions for both source and destination buckets. - Monitor the Lambda executions via CloudWatch for errors and duration.</p>"},{"location":"data-modeling/","title":"Data Modeling &amp; Warehousing","text":""},{"location":"data-modeling/#1-star-vs-snowflake-schema","title":"1. Star vs Snowflake Schema","text":"<p>Question: When should you use Star Schema vs Snowflake Schema in a data warehouse?</p> <p>Answer: - Use Star Schema when you prioritize performance and have denormalized dimensions. - Use Snowflake Schema when space optimization, flexibility, or easier maintenance is important.</p>"},{"location":"data-modeling/#2-slowly-changing-dimensions-scd","title":"2. Slowly Changing Dimensions (SCD)","text":"<p>Question: How do you implement SCD Type 2 in a data warehouse?</p> <p>Answer: - Maintain historical records by adding <code>start_date</code>, <code>end_date</code>, and <code>is_current</code> columns. - Use ETL tools (Glue, dbt, etc.) or SQL MERGE logic to detect changes and insert new rows.</p>"},{"location":"projects/","title":"Data Engineering Projects","text":""},{"location":"projects/#projects-overview","title":"Projects Overview","text":""},{"location":"projects/#1-retail-analytics-platform","title":"1. Retail Analytics Platform","text":"<p>Status: Planned Stack: S3, Glue, dbt, Snowflake, Power BI  </p> <p>Goal: Build a retail analytics platform to analyze customer behavior and sales performance. Architecture (Planned): - Data Ingestion: Raw sales/customer data \u2192 S3 - ETL: AWS Glue jobs and dbt transformations - Data Warehouse: Snowflake - Reporting Layer: Power BI dashboards</p> <p>Expected Outcome: - Real-time KPIs: revenue, churn rate, AOV - Product-wise and region-wise performance insights - Inventory and demand forecasting support</p>"},{"location":"projects/#2-iot-monitoring-maintenance","title":"2. IoT Monitoring + Maintenance","text":"<p>Status: Planned Stack: Kinesis, Lambda, DynamoDB  </p> <p>Goal: Monitor real-time sensor data from manufacturing machines to predict maintenance needs and avoid downtime. Architecture (Planned): - Streaming ingestion via AWS Kinesis - Event processing with AWS Lambda - Real-time status and alerts stored in DynamoDB - Visualization dashboard for plant supervisors</p> <p>Expected Outcome: - Real-time alerts on anomaly detection - Predictive maintenance analytics - Uptime/downtime trend analysis</p>"},{"location":"pyspark/","title":"PySpark Coding Questions","text":"<p>Below are some medium-complexity PySpark problems commonly asked in data engineering interviews.</p>"},{"location":"pyspark/#1-group-customers-by-first-purchase-category","title":"1. Group Customers by First Purchase Category","text":"<p>Problem: Given a dataset of customer purchases, write a PySpark program to find the first product category each customer purchased.</p> <p>Input Schema: <code>customer_id</code>, <code>purchase_date</code>, <code>category</code></p> <p>Expected Output: <code>customer_id</code>, <code>first_category</code></p> <pre><code>import pyspark.sql.functions as F\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"purchase_date\")\ndf_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\nfirst_purchase = df_with_rank.filter(F.col(\"rank\") == 1).select(\"customer_id\", \"category\").withColumnRenamed(\"category\", \"first_category\")\n</code></pre>"},{"location":"pyspark/#2-rolling-7-day-aggregation-of-page-views","title":"2. Rolling 7-Day Aggregation of Page Views","text":"<p>Problem: Given a dataset of web page views, compute the rolling 7-day total views per user.</p> <p>Input Schema: <code>user_id</code>, <code>view_date</code>, <code>page_url</code></p> <p>Expected Output: <code>user_id</code>, <code>view_date</code>, <code>rolling_7_day_views</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"view_date\", F.to_date(\"view_date\"))\ndf = df.withColumn(\"view_date_ts\", F.col(\"view_date\").cast(\"timestamp\").cast(\"long\"))\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"view_date_ts\").rangeBetween(-6 * 86400, 0)\ndaily_views = df.groupBy(\"user_id\", \"view_date\").agg(F.count(\"*\").alias(\"daily_views\"))\nrolling_views = daily_views.withColumn(\"rolling_7_day_views\", F.sum(\"daily_views\").over(window_spec))\n</code></pre>"},{"location":"pyspark/#3-detect-session-start-and-end-for-user-activity","title":"3. Detect Session Start and End for User Activity","text":"<p>Problem: Given a timestamped list of user actions, detect sessions for each user. A new session starts if the gap between two actions is more than 30 minutes.</p> <p>Input Schema: <code>user_id</code>, <code>timestamp</code>, <code>event_type</code></p> <p>Expected Output: <code>user_id</code>, <code>session_id</code>, <code>session_start</code>, <code>session_end</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\ndf = df.withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(window_spec))\ndf = df.withColumn(\"session_gap\", F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\"))\ndf = df.withColumn(\"is_new_session\", (F.col(\"session_gap\") &gt; 1800) | F.col(\"session_gap\").isNull())\nwindow_spec_session = window_spec.rowsBetween(Window.unboundedPreceding, 0)\ndf = df.withColumn(\"session_id\", F.sum(F.col(\"is_new_session\").cast(\"int\")).over(window_spec_session))\n\nsession_boundaries = df.groupBy(\"user_id\", \"session_id\").agg(\n    F.min(\"timestamp\").alias(\"session_start\"),\n    F.max(\"timestamp\").alias(\"session_end\")\n)\n</code></pre>"},{"location":"pyspark/#4-identify-top-n-products-per-category-by-revenue","title":"4. Identify Top N Products Per Category by Revenue","text":"<p>Problem: Given a dataset of product sales, calculate revenue per product and return top 3 products in each category.</p> <p>Input Schema: <code>product_id</code>, <code>category</code>, <code>unit_price</code>, <code>quantity</code></p> <p>Expected Output: <code>category</code>, <code>product_id</code>, <code>revenue</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"revenue\", F.col(\"unit_price\") * F.col(\"quantity\"))\nwindow_spec = Window.partitionBy(\"category\").orderBy(F.desc(\"revenue\"))\ndf = df.withColumn(\"rank\", F.row_number().over(window_spec))\ntop_n_products = df.filter(F.col(\"rank\") &lt;= 3).select(\"category\", \"product_id\", \"revenue\")\n</code></pre> <p>Note: You can also use <code>dense_rank()</code> or <code>rank()</code> depending on how you want to treat ties in revenue.</p>"},{"location":"python/","title":"Python","text":"<p>title: Python Interview Questions description: A curated list of Python coding questions including medium-level problems and string manipulation challenges, commonly asked in data engineering and software interviews. keywords: Python, interview questions, coding problems, string algorithms, LeetCode, Python strings</p>"},{"location":"python/#python-interview-questions","title":"Python Interview Questions","text":""},{"location":"python/#medium-complexity-problems","title":"Medium Complexity Problems","text":""},{"location":"python/#1-longest-substring-without-repeating-characters","title":"1. Longest Substring Without Repeating Characters","text":"<p>Problem: Given a string, find the length of the longest substring without repeating characters. Approach: Use a sliding window and a hash set to keep track of characters in the current window. Code: <pre><code>def length_of_longest_substring(s: str) -&gt; int:\n    char_set = set()\n    left = 0\n    max_len = 0\n\n    for right in range(len(s)):\n        while s[right] in char_set:\n            char_set.remove(s[left])\n            left += 1\n        char_set.add(s[right])\n        max_len = max(max_len, right - left + 1)\n    return max_len\n</code></pre></p>"},{"location":"python/#2-3sum","title":"2. 3Sum","text":"<p>Problem: Given an array <code>nums</code> of <code>n</code> integers, find all unique triplets in the array which gives the sum of zero. Approach: Sort the array and use a two-pointer approach inside a loop. Code: <pre><code>def three_sum(nums: list[int]) -&gt; list[list[int]]:\n    nums.sort()\n    res = []\n    for i in range(len(nums) - 2):\n        if i &gt; 0 and nums[i] == nums[i - 1]:\n            continue\n        left, right = i + 1, len(nums) - 1\n        while left &lt; right:\n            s = nums[i] + nums[left] + nums[right]\n            if s &lt; 0:\n                left += 1\n            elif s &gt; 0:\n                right -= 1\n            else:\n                res.append([nums[i], nums[left], nums[right]])\n                while left &lt; right and nums[left] == nums[left + 1]:\n                    left += 1\n                while left &lt; right and nums[right] == nums[right - 1]:\n                    right -= 1\n                left += 1\n                right -= 1\n    return res\n</code></pre></p>"},{"location":"python/#string-related-questions","title":"String-Related Questions","text":""},{"location":"python/#1-reverse-words-in-a-string","title":"1. Reverse Words in a String","text":"<p>Problem: Given an input string <code>s</code>, reverse the order of the words. Code: <pre><code>def reverse_words(s: str) -&gt; str:\n    return \" \".join(reversed(s.strip().split()))\n</code></pre></p>"},{"location":"python/#2-valid-anagram","title":"2. Valid Anagram","text":"<p>Problem: Given two strings <code>s</code> and <code>t</code>, return true if <code>t</code> is an anagram of <code>s</code>, and false otherwise. Code: <pre><code>def is_anagram(s: str, t: str) -&gt; bool:\n    return sorted(s) == sorted(t)\n</code></pre></p>"},{"location":"sql/","title":"SQL Interview Questions","text":""},{"location":"sql/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ol> <li>What is a LEFT ANTI JOIN?</li> <li>Find Second Highest Salary</li> <li>WHERE vs HAVING</li> <li>Find Duplicate Emails</li> <li>Latest Record Per Group</li> <li>Running Total of Sales</li> <li>Employees Earning More Than Manager</li> <li>Employees Hired Last 6 Months</li> <li>Running Salary Total</li> <li>Pivot Monthly Sales</li> </ol> <p>\u2139\ufe0f All queries use ANSI SQL unless a specific dialect (e.g., PostgreSQL, MySQL, SQL Server) is mentioned.</p>"},{"location":"sql/#sql-interview-questions","title":"SQL Interview Questions","text":"<p>\ud83d\udfe2 Difficulty: Easy\u2003\u2003\ud83d\udcc2 Topic: Joins</p>"},{"location":"sql/#q1-what-is-a-left-anti-join-how-is-it-different-from-left-join","title":"Q1. What is a LEFT ANTI JOIN? How is it different from LEFT JOIN?","text":"<p>A LEFT ANTI JOIN returns only the rows from the left table that do not have a match in the right table.</p> <p>This is different from a LEFT JOIN, which returns all rows from the left table and matching rows from the right (NULL if no match).</p>"},{"location":"sql/#table-structure-sample-data","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>orders</code> and <code>customers</code> tables:</p> <p>orders</p> id customer_id order_date 1 101 2023-01-10 2 102 2023-01-11 3 NULL 2023-01-12 <p>customers</p> id name 101 Alice 102 Bob"},{"location":"sql/#example","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT *\nFROM orders o\nLEFT ANTI JOIN customers c\nON o.customer_id = c.id;\n</code></pre> <p>This query helps identify orphaned records \u2014 such as orders that are not linked to any customer.</p>"},{"location":"sql/#key-insight","title":"\ud83d\udca1 Key Insight:","text":"<p>LEFT ANTI JOIN effectively filters out all rows from the left table that have matching rows in the right table, returning only unmatched rows.</p>"},{"location":"sql/#expected-output","title":"\ud83e\uddfe Expected Output:","text":"id customer_id order_date 3 NULL 2023-01-12 <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #joins #anti-join</p> <p>\ud83d\udfe1 Difficulty: Medium\u2003\u2003\ud83d\udcc2 Topic: Subqueries</p>"},{"location":"sql/#q2-write-a-sql-query-to-find-the-second-highest-salary-from-an-employees-table","title":"Q2. Write a SQL query to find the second highest salary from an <code>employees</code> table.","text":""},{"location":"sql/#table-structure-sample-data_1","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>employees</code> table:</p> <p>employees</p> id name salary department hire_date manager_id 1 Alice 70000 HR 2021-05-01 NULL 2 Bob 90000 IT 2020-03-15 1 3 Carol 80000 IT 2022-01-10 2 4 Dave 60000 HR 2023-04-20 1"},{"location":"sql/#example_1","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT MAX(salary) AS second_highest\nFROM employees\nWHERE salary &lt; (\n  SELECT MAX(salary) FROM employees\n);\n</code></pre> <p>This uses a subquery to exclude the highest salary and returns the next highest.</p>"},{"location":"sql/#key-insight_1","title":"\ud83d\udca1 Key Insight:","text":"<p>This query demonstrates how to find the second highest value using a subquery that excludes the maximum.</p>"},{"location":"sql/#expected-output_1","title":"\ud83e\uddfe Expected Output:","text":"second_highest 80000 <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #subquery #ranking</p> <p>\ud83d\udfe2 Difficulty: Easy\u2003\u2003\ud83d\udcc2 Topic: Aggregation</p>"},{"location":"sql/#q3-what-is-the-difference-between-where-and-having-in-sql","title":"Q3. What is the difference between <code>WHERE</code> and <code>HAVING</code> in SQL?","text":"<ul> <li><code>WHERE</code> filters rows before aggregation.</li> <li><code>HAVING</code> filters after aggregation.</li> </ul>"},{"location":"sql/#table-structure-sample-data_2","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>employees</code> table:</p> <p>employees</p> id name salary department hire_date 1 Alice 50000 HR 2022-02-15 2 Bob 70000 IT 2021-06-12 3 Charlie 60000 IT 2023-01-20 4 Diana 55000 Finance 2020-11-05"},{"location":"sql/#example_2","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT department, COUNT(*) AS emp_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) &gt; 5;\n</code></pre> <p>This query filters departments with more than 5 employees \u2014 <code>HAVING</code> is used after the <code>GROUP BY</code>.</p>"},{"location":"sql/#key-insight_2","title":"\ud83d\udca1 Key Insight:","text":"<p>Use WHERE to filter rows before aggregation, and HAVING to filter groups after aggregation.</p>"},{"location":"sql/#expected-output_2","title":"\ud83e\uddfe Expected Output:","text":"department emp_count (no rows) <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #aggregation #filtering</p> <p>\ud83d\udfe2 Difficulty: Easy\u2003\u2003\ud83d\udcc2 Topic: Duplicates</p>"},{"location":"sql/#q4-write-a-query-to-find-duplicate-records-based-on-email-in-a-users-table","title":"Q4. Write a query to find duplicate records based on <code>email</code> in a <code>users</code> table.","text":""},{"location":"sql/#table-structure-sample-data_3","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>users</code> table:</p> <p>users</p> id name email 1 Alice alice@email.com 2 Bob bob@email.com 3 Carol alice@email.com 4 Dave dave@email.com 5 Eve eve@email.com 6 Frank bob@email.com"},{"location":"sql/#example_3","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT email, COUNT(*) AS count\nFROM users\nGROUP BY email\nHAVING COUNT(*) &gt; 1;\n</code></pre> <p>This helps identify duplicates that can be cleaned or deduplicated.</p>"},{"location":"sql/#key-insight_3","title":"\ud83d\udca1 Key Insight:","text":"<p>Grouping by email and filtering with HAVING identifies duplicate email entries in the dataset.</p>"},{"location":"sql/#expected-output_3","title":"\ud83e\uddfe Expected Output:","text":"email count alice@email.com 2 bob@email.com 2 <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #duplicates #group-by</p> <p>\ud83d\udfe1 Difficulty: Medium\u2003\u2003\ud83d\udcc2 Topic: Grouping and Filtering</p>"},{"location":"sql/#q5-how-do-you-retrieve-the-latest-top-1-record-per-group-in-sql","title":"Q5. How do you retrieve the latest (Top 1) record per group in SQL?","text":"<p>This is useful when you want to get the most recent order per customer, highest sale per region, etc.</p>"},{"location":"sql/#table-structure-sample-data_4","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>orders</code> table:</p> <p>orders</p> id customer_id order_date amount 1 101 2023-01-10 250 2 102 2023-01-11 100 3 101 2023-02-12 400 4 103 2023-02-15 150 5 102 2023-03-01 200"},{"location":"sql/#example-get-the-most-recent-order-per-customer-from-an-orders-table","title":"\ud83d\udd0d Example: Get the most recent order per customer from an <code>orders</code> table","text":"<pre><code>SELECT o.*\nFROM orders o\nJOIN (\n    SELECT customer_id, MAX(order_date) AS latest_order\n    FROM orders\n    GROUP BY customer_id\n) latest\nON o.customer_id = latest.customer_id\nAND o.order_date = latest.latest_order;\n</code></pre> <p>This query uses a self-join to fetch the latest order for each customer based on <code>order_date</code>.</p>"},{"location":"sql/#key-insight_4","title":"\ud83d\udca1 Key Insight:","text":"<p>A self-join on max date per group efficiently retrieves the latest record per group.</p>"},{"location":"sql/#expected-output_4","title":"\ud83e\uddfe Expected Output:","text":"id customer_id order_date amount 3 101 2023-02-12 400 5 102 2023-03-01 200 4 103 2023-02-15 150"},{"location":"sql/#alternative-use-row_number-if-supported-by-your-sql-dialect","title":"\u2705 Alternative: Use <code>ROW_NUMBER()</code> (if supported by your SQL dialect)","text":"<pre><code>SELECT *\nFROM (\n  SELECT *,\n         ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS rn\n  FROM orders\n) ranked\nWHERE rn = 1;\n</code></pre> <p>This approach assigns a row number to each order within a customer partition and selects only the top-ranked (most recent) row.</p>"},{"location":"sql/#key-insight_5","title":"\ud83d\udca1 Key Insight:","text":"<p>Using ROW_NUMBER() allows retrieval of the top record per group without self-joins.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #window-functions #latest-record</p> <p>\ud83d\udfe0 Difficulty: Medium-High\u2003\u2003\ud83d\udcc2 Topic: Window Functions</p>"},{"location":"sql/#q6-write-a-query-to-calculate-the-running-cumulative-total-of-sales-per-day","title":"Q6. Write a query to calculate the running (cumulative) total of sales per day.","text":"<p>This is commonly asked to test your understanding of window functions and date-wise aggregations.</p>"},{"location":"sql/#table-structure-sample-data_5","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>sales</code> table:</p> <p>sales</p> id sale_date amount region 1 2023-01-01 100 North 2 2023-01-02 200 North 3 2023-01-01 150 South 4 2023-01-03 250 North 5 2023-01-02 120 South"},{"location":"sql/#example-from-a-sales-table-with-columns-sale_date-and-amount","title":"\ud83d\udd0d Example: From a <code>sales</code> table with columns <code>sale_date</code> and <code>amount</code>","text":"<pre><code>SELECT\n  sale_date,\n  amount,\n  SUM(amount) OVER (ORDER BY sale_date) AS running_total\nFROM sales;\n</code></pre> <p>\ud83d\udcdd Note: This syntax works in PostgreSQL. For MySQL, refer to the dialect-specific version below.</p> <p>This query calculates a cumulative sum of <code>amount</code> ordered by <code>sale_date</code>.</p>"},{"location":"sql/#key-insight_6","title":"\ud83d\udca1 Key Insight:","text":"<p>Window functions enable cumulative calculations over ordered data without collapsing rows.</p>"},{"location":"sql/#expected-output_5","title":"\ud83e\uddfe Expected Output:","text":"sale_date amount running_total 2023-01-01 100 100 2023-01-01 150 250 2023-01-02 200 450 2023-01-02 120 570 2023-01-03 250 820"},{"location":"sql/#alternative-partition-by-another-field-eg-region","title":"\u2705 Alternative: Partition by another field (e.g., region)","text":"<pre><code>SELECT\n  region,\n  sale_date,\n  amount,\n  SUM(amount) OVER (PARTITION BY region ORDER BY sale_date) AS regional_running_total\nFROM sales;\n</code></pre> <p>This gives a running total per region per day.</p>"},{"location":"sql/#key-insight_7","title":"\ud83d\udca1 Key Insight:","text":"<p>Partitioning window functions allows separate running totals within categories.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #window-functions #cumulative-sum</p> <p>\ud83d\udfe1 Difficulty: Medium\u2003\u2003\ud83d\udcc2 Topic: Self Joins</p>"},{"location":"sql/#q7-write-a-sql-query-to-find-employees-who-earn-more-than-their-manager","title":"Q7. Write a SQL query to find employees who earn more than their manager.","text":"<p>This question tests understanding of self-joins and hierarchical data.</p> <p>Assume the <code>employees</code> table has the following columns: - <code>id</code>: Employee ID - <code>name</code>: Employee name - <code>salary</code>: Employee salary - <code>manager_id</code>: Manager\u2019s employee ID</p>"},{"location":"sql/#table-structure-sample-data_6","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>employees</code> table:</p> <p>employees</p> id name salary manager_id 1 Alice 90000 NULL 2 Bob 85000 1 3 Carol 95000 2 4 Dave 80000 1 5 Eve 99000 2"},{"location":"sql/#example_4","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT e.name AS employee_name, e.salary AS employee_salary,\n       m.name AS manager_name, m.salary AS manager_salary\nFROM employees e\nJOIN employees m\n  ON e.manager_id = m.id\nWHERE e.salary &gt; m.salary;\n</code></pre> <p>This query joins the <code>employees</code> table to itself: - <code>e</code> represents employees - <code>m</code> represents their managers</p> <p>The filter <code>e.salary &gt; m.salary</code> ensures we return only those employees earning more than their respective manager.</p>"},{"location":"sql/#key-insight_8","title":"\ud83d\udca1 Key Insight:","text":"<p>Self-joins allow comparing hierarchical relationships such as employee and manager salaries.</p>"},{"location":"sql/#expected-output_6","title":"\ud83e\uddfe Expected Output:","text":"employee_name employee_salary manager_name manager_salary Carol 95000 Bob 85000 Eve 99000 Bob 85000"},{"location":"sql/#alternative-select-only-employee-and-manager-ids-for-a-simplified-result","title":"\u2705 Alternative: Select only employee and manager IDs for a simplified result","text":"<pre><code>SELECT e.id AS employee_id, m.id AS manager_id\nFROM employees e\nJOIN employees m\n  ON e.manager_id = m.id\nWHERE e.salary &gt; m.salary;\n</code></pre> <p>This version returns just the IDs for use in further processing.</p>"},{"location":"sql/#key-insight_9","title":"\ud83d\udca1 Key Insight:","text":"<p>Selecting IDs can simplify output when only identifiers are needed for further queries.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #self-join #hierarchical-data</p> <p>\ud83d\udfe2 Difficulty: Easy\u2003\u2003\ud83d\udcc2 Topic: Date Filtering</p>"},{"location":"sql/#q8-write-a-sql-query-to-find-employees-hired-in-the-last-6-months","title":"Q8. Write a SQL query to find employees hired in the last 6 months.","text":"<p>This question tests your ability to work with date functions and interval calculations.</p> <p>Assume the <code>employees</code> table has a column: - <code>hire_date</code>: the date when the employee was hired.</p>"},{"location":"sql/#table-structure-sample-data_7","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>employees</code> table:</p> <p>employees</p> id name hire_date department 1 Alice 2023-12-01 HR 2 Bob 2023-04-15 IT 3 Carol 2022-10-10 IT 4 Dave 2023-09-05 Finance 5 Eve 2022-07-20 HR"},{"location":"sql/#example_5","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT *\nFROM employees\nWHERE hire_date &gt;= CURRENT_DATE - INTERVAL '6 months';\n</code></pre> <p>\ud83d\udcdd Note: This syntax works in PostgreSQL. For MySQL, refer to the dialect-specific version below.</p> <p>This query retrieves all employees whose <code>hire_date</code> is within the past 6 months from the current date.</p>"},{"location":"sql/#key-insight_10","title":"\ud83d\udca1 Key Insight:","text":"<p>Date arithmetic with INTERVAL helps filter recent records relative to the current date.</p>"},{"location":"sql/#expected-output_7","title":"\ud83e\uddfe Expected Output:","text":"id name hire_date department 1 Alice 2023-12-01 HR 4 Dave 2023-09-05 Finance"},{"location":"sql/#alternative-for-mysql","title":"\u2705 Alternative (for MySQL):","text":"<pre><code>SELECT *\nFROM employees\nWHERE hire_date &gt;= DATE_SUB(CURDATE(), INTERVAL 6 MONTH);\n</code></pre> <p>Use the appropriate syntax based on your SQL dialect.</p>"},{"location":"sql/#key-insight_11","title":"\ud83d\udca1 Key Insight:","text":"<p>MySQL uses DATE_SUB and CURDATE() for date interval calculations similar to PostgreSQL's INTERVAL.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #date-functions #interval</p> <p>\ud83d\udfe0 Difficulty: Medium-High\u2003\u2003\ud83d\udcc2 Topic: Running Totals</p>"},{"location":"sql/#q9-write-a-sql-query-to-calculate-the-running-total-of-employee-salaries","title":"Q9. Write a SQL query to calculate the running total of employee salaries.","text":"<p>This question checks your understanding of window functions and cumulative aggregations.</p> <p>Assume the <code>employees</code> table has the following structure:</p> id name department salary hire_date 1 Alice HR 50000 2022-02-15 2 Bob IT 70000 2021-06-12 3 Charlie IT 60000 2023-01-20 4 Diana Finance 55000 2020-11-05"},{"location":"sql/#example-calculate-cumulative-salary-ordered-by-hire-date","title":"\ud83d\udd0d Example: Calculate cumulative salary ordered by hire date","text":"<pre><code>SELECT\n  id,\n  name,\n  salary,\n  hire_date,\n  SUM(salary) OVER (ORDER BY hire_date) AS running_salary_total\nFROM employees;\n</code></pre> <p>This query gives the running total of <code>salary</code> ordered by <code>hire_date</code> \u2014 useful for analyzing salary expense trends over time.</p>"},{"location":"sql/#key-insight_12","title":"\ud83d\udca1 Key Insight:","text":"<p>Window functions can calculate cumulative totals ordered by a specific column, like hire date.</p>"},{"location":"sql/#expected-output_8","title":"\ud83e\uddfe Expected Output:","text":"id name salary hire_date running_salary_total 4 Diana 55000 2020-11-05 55000 2 Bob 70000 2021-06-12 125000 1 Alice 50000 2022-02-15 175000 3 Charlie 60000 2023-01-20 235000"},{"location":"sql/#alternative-partition-by-department","title":"\u2705 Alternative: Partition by department","text":"<pre><code>SELECT\n  id,\n  name,\n  department,\n  salary,\n  hire_date,\n  SUM(salary) OVER (PARTITION BY department ORDER BY hire_date) AS dept_running_salary\nFROM employees;\n</code></pre> <p>This version gives the cumulative salary per department, ordered by hiring date.</p>"},{"location":"sql/#key-insight_13","title":"\ud83d\udca1 Key Insight:","text":"<p>Partitioning by department calculates running totals separately for each department.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #window-functions #cumulative-sum</p> <p>\ud83d\udfe1 Difficulty: Medium\u2003\u2003\ud83d\udcc2 Topic: Pivoting</p>"},{"location":"sql/#q10-write-a-sql-query-to-pivot-monthly-sales-data-by-product","title":"Q10. Write a SQL query to pivot monthly sales data by product.","text":"<p>This question tests your understanding of pivoting, aggregation, and conditional expressions.</p>"},{"location":"sql/#table-structure-sample-data_8","title":"\ud83d\udccb Table Structure &amp; Sample Data","text":"<p>Assume we have the following <code>sales</code> table:</p> <p>sales</p> id product sale_month amount 1 Laptop Jan 1000 2 Laptop Feb 1500 3 Monitor Jan 700 4 Monitor Feb 900 5 Keyboard Jan 300 6 Keyboard Feb 400"},{"location":"sql/#example_6","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT\n  product,\n  SUM(CASE WHEN sale_month = 'Jan' THEN amount ELSE 0 END) AS Jan,\n  SUM(CASE WHEN sale_month = 'Feb' THEN amount ELSE 0 END) AS Feb\nFROM sales\nGROUP BY product;\n</code></pre> <p>This query pivots <code>sale_month</code> into separate columns and aggregates <code>amount</code> for each product.</p>"},{"location":"sql/#key-insight_14","title":"\ud83d\udca1 Key Insight:","text":"<p>Conditional aggregation enables pivoting data without special pivot functions.</p>"},{"location":"sql/#expected-output_9","title":"\ud83e\uddfe Expected Output:","text":"product Jan Feb Laptop 1000 1500 Monitor 700 900 Keyboard 300 400"},{"location":"sql/#alternative-using-pivot-sql-server-oracle","title":"\u2705 Alternative (Using PIVOT \u2014 SQL Server / Oracle):","text":"<pre><code>SELECT *\nFROM (\n  SELECT product, sale_month, amount\n  FROM sales\n) src\nPIVOT (\n  SUM(amount)\n  FOR sale_month IN ([Jan], [Feb])\n) AS p;\n</code></pre> <p>This approach uses built-in pivot functionality available in some SQL dialects.</p>"},{"location":"sql/#key-insight_15","title":"\ud83d\udca1 Key Insight:","text":"<p>Pivot syntax simplifies transforming row data into columns but is dialect-specific.</p> <p>\u2190 Previous Question | Next Question \u2192</p> <p>Tags: #pivot #aggregation #conditional-aggregation</p>"},{"location":"system-design/","title":"System Design Questions","text":""},{"location":"system-design/#1-how-do-you-design-a-scalable-real-time-analytics-platform","title":"1. How do you design a scalable real-time analytics platform?","text":"<p>A real-time analytics platform typically includes: - Data Ingestion: Kafka / Kinesis - Stream Processing: Apache Flink / Spark Streaming - Storage: Hudi / Iceberg / Druid / ClickHouse - Serving Layer: Presto / Athena / API with caching - Ensure horizontal scalability, exactly-once semantics, and low latency.</p>"},{"location":"system-design/#2-what-are-the-core-tradeoffs-in-batch-vs-stream-processing","title":"2. What are the core tradeoffs in batch vs stream processing?","text":"Aspect Batch Processing Stream Processing Latency High (minutes to hours) Low (seconds to milliseconds) Data Freshness Delayed insights Real-time insights Tools Spark, dbt, Airflow Flink, Kafka Streams, Beam Use Cases Reports, ETL Monitoring, anomaly detection"},{"location":"system-design/#3-how-do-you-design-a-data-lakehouse","title":"3. How do you design a data lakehouse?","text":"<p>Key components: - Storage: S3/GCS + table formats like Apache Iceberg or Delta Lake - Metadata Layer: Catalog (Glue, Hive, Unity Catalog) - Processing Engine: Spark / Dask / Trino - Features:   - ACID transactions   - Schema evolution   - Time travel - Ensure separation of storage and compute, and support for both BI + ML workloads.</p>"}]}