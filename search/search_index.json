{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to BytePrep \u2013 Your Data Engineering Interview Hub \ud83d\ude80","text":"<p>This is a curated, no-fluff guide for preparing for Data Engineering interviews \u2014 covering topics like:</p> <ul> <li>\u2705 SQL and query optimization</li> <li>\u2705 Python for data engineering</li> <li>\u2705 PySpark and distributed processing</li> <li>\u2705 AWS and cloud services</li> <li>\u2705 System design for data-intensive systems</li> <li>\u2705 Behavioral and scenario-based questions</li> </ul> <p>This is a live and growing collection. Stay consistent, practice daily, and land your dream DE role!</p> <p>Use the tabs above to start exploring by topic. \u2728</p>"},{"location":"about/","title":"About BytePrep","text":""},{"location":"about/#about-byteprep","title":"About BytePrep","text":"<p>BytePrep is a curated, no-fluff resource built by Anand Kumar Singh to help aspiring and experienced data engineers prepare for technical interviews with confidence.</p> <p>This guide brings together real-world, scenario-based questions across essential domains like SQL, Python, PySpark, AWS, system design, data modeling, and more.</p> <p>Built using MkDocs and the Material for MkDocs theme, the site is designed to be fast, searchable, and content-rich \u2014 without distractions.</p> <p>\ud83d\udccc This is an evolving resource. New questions, patterns, and project examples will be continuously added. Stay consistent, and good luck on your data engineering journey!</p>"},{"location":"aws/","title":"AWS","text":"<p>title: AWS Interview Questions description: Explore real-world AWS scenario-based questions covering Glue, S3, Lambda, and best practices for data engineering interviews.</p>"},{"location":"aws/#aws-scenario-based-questions","title":"AWS Scenario-Based Questions","text":""},{"location":"aws/#1-aws-glue-scenario","title":"1. AWS Glue Scenario","text":"<p>Question: You have a daily batch job that ingests CSV files from an S3 bucket, performs transformations, and writes to a Snowflake data warehouse. Occasionally, the schema of incoming files changes (e.g., new columns are added). How do you handle schema evolution in AWS Glue?</p> <p>Answer: - Use AWS Glue DynamicFrame with <code>mergeSchema=True</code> to handle schema evolution. - Create a Glue Crawler that runs before the ETL job to update the Data Catalog. - In PySpark code, dynamically reference columns using <code>df.columns</code> and use conditional logic (<code>if 'new_col' in df.columns</code>) for optional fields. - Update the Snowflake destination table with new columns and use <code>MERGE</code> logic to maintain idempotency.</p>"},{"location":"aws/#2-amazon-s3-scenario","title":"2. Amazon S3 Scenario","text":"<p>Question: You are storing large amounts of data in S3 that are accessed infrequently but must be retrieved within a few minutes when needed. You also want to reduce costs. Which storage class should you use and how will you automate the transition?</p> <p>Answer: - Use S3 Intelligent-Tiering or S3 Standard-IA (Infrequent Access) based on access patterns. - Set up an S3 Lifecycle policy to transition objects to the desired storage class after a specific number of days (e.g., 30 days). - Enable object-level logging and monitoring via CloudWatch to refine lifecycle rules over time.</p>"},{"location":"aws/#3-aws-lambda-scenario","title":"3. AWS Lambda Scenario","text":"<p>Question: You have an S3 bucket that receives real-time uploads of images. You want to resize and store them in another bucket as thumbnails. How do you implement this using AWS Lambda?</p> <p>Answer: - Configure the source S3 bucket to trigger a Lambda function on <code>s3:ObjectCreated</code> events. - The Lambda function (using Python or Node.js) reads the image using <code>boto3</code>, processes it using <code>Pillow</code> or <code>Sharp</code>, and writes the thumbnail to a destination S3 bucket. - Ensure the Lambda has appropriate IAM permissions for both source and destination buckets. - Monitor the Lambda executions via CloudWatch for errors and duration.</p>"},{"location":"data-modeling/","title":"Data Modeling &amp; Warehousing","text":""},{"location":"data-modeling/#1-star-vs-snowflake-schema","title":"1. Star vs Snowflake Schema","text":"<p>Question: When should you use Star Schema vs Snowflake Schema in a data warehouse?</p> <p>Answer: - Use Star Schema when you prioritize performance and have denormalized dimensions. - Use Snowflake Schema when space optimization, flexibility, or easier maintenance is important.</p>"},{"location":"data-modeling/#2-slowly-changing-dimensions-scd","title":"2. Slowly Changing Dimensions (SCD)","text":"<p>Question: How do you implement SCD Type 2 in a data warehouse?</p> <p>Answer: - Maintain historical records by adding <code>start_date</code>, <code>end_date</code>, and <code>is_current</code> columns. - Use ETL tools (Glue, dbt, etc.) or SQL MERGE logic to detect changes and insert new rows.</p>"},{"location":"projects/","title":"Data Engineering Projects","text":""},{"location":"projects/#projects-overview","title":"Projects Overview","text":""},{"location":"projects/#1-retail-analytics-platform","title":"1. Retail Analytics Platform","text":"<p>Status: Planned Stack: S3, Glue, dbt, Snowflake, Power BI  </p> <p>Goal: Build a retail analytics platform to analyze customer behavior and sales performance. Architecture (Planned): - Data Ingestion: Raw sales/customer data \u2192 S3 - ETL: AWS Glue jobs and dbt transformations - Data Warehouse: Snowflake - Reporting Layer: Power BI dashboards</p> <p>Expected Outcome: - Real-time KPIs: revenue, churn rate, AOV - Product-wise and region-wise performance insights - Inventory and demand forecasting support</p>"},{"location":"projects/#2-iot-monitoring-maintenance","title":"2. IoT Monitoring + Maintenance","text":"<p>Status: Planned Stack: Kinesis, Lambda, DynamoDB  </p> <p>Goal: Monitor real-time sensor data from manufacturing machines to predict maintenance needs and avoid downtime. Architecture (Planned): - Streaming ingestion via AWS Kinesis - Event processing with AWS Lambda - Real-time status and alerts stored in DynamoDB - Visualization dashboard for plant supervisors</p> <p>Expected Outcome: - Real-time alerts on anomaly detection - Predictive maintenance analytics - Uptime/downtime trend analysis</p>"},{"location":"pyspark/","title":"PySpark Coding Questions","text":"<p>Below are some medium-complexity PySpark problems commonly asked in data engineering interviews.</p>"},{"location":"pyspark/#1-group-customers-by-first-purchase-category","title":"1. Group Customers by First Purchase Category","text":"<p>Problem: Given a dataset of customer purchases, write a PySpark program to find the first product category each customer purchased.</p> <p>Input Schema: <code>customer_id</code>, <code>purchase_date</code>, <code>category</code></p> <p>Expected Output: <code>customer_id</code>, <code>first_category</code></p> <pre><code>import pyspark.sql.functions as F\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"purchase_date\")\ndf_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\nfirst_purchase = df_with_rank.filter(F.col(\"rank\") == 1).select(\"customer_id\", \"category\").withColumnRenamed(\"category\", \"first_category\")\n</code></pre>"},{"location":"pyspark/#2-rolling-7-day-aggregation-of-page-views","title":"2. Rolling 7-Day Aggregation of Page Views","text":"<p>Problem: Given a dataset of web page views, compute the rolling 7-day total views per user.</p> <p>Input Schema: <code>user_id</code>, <code>view_date</code>, <code>page_url</code></p> <p>Expected Output: <code>user_id</code>, <code>view_date</code>, <code>rolling_7_day_views</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"view_date\", F.to_date(\"view_date\"))\ndf = df.withColumn(\"view_date_ts\", F.col(\"view_date\").cast(\"timestamp\").cast(\"long\"))\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"view_date_ts\").rangeBetween(-6 * 86400, 0)\ndaily_views = df.groupBy(\"user_id\", \"view_date\").agg(F.count(\"*\").alias(\"daily_views\"))\nrolling_views = daily_views.withColumn(\"rolling_7_day_views\", F.sum(\"daily_views\").over(window_spec))\n</code></pre>"},{"location":"pyspark/#3-detect-session-start-and-end-for-user-activity","title":"3. Detect Session Start and End for User Activity","text":"<p>Problem: Given a timestamped list of user actions, detect sessions for each user. A new session starts if the gap between two actions is more than 30 minutes.</p> <p>Input Schema: <code>user_id</code>, <code>timestamp</code>, <code>event_type</code></p> <p>Expected Output: <code>user_id</code>, <code>session_id</code>, <code>session_start</code>, <code>session_end</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\ndf = df.withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(window_spec))\ndf = df.withColumn(\"session_gap\", F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\"))\ndf = df.withColumn(\"is_new_session\", (F.col(\"session_gap\") &gt; 1800) | F.col(\"session_gap\").isNull())\nwindow_spec_session = window_spec.rowsBetween(Window.unboundedPreceding, 0)\ndf = df.withColumn(\"session_id\", F.sum(F.col(\"is_new_session\").cast(\"int\")).over(window_spec_session))\n\nsession_boundaries = df.groupBy(\"user_id\", \"session_id\").agg(\n    F.min(\"timestamp\").alias(\"session_start\"),\n    F.max(\"timestamp\").alias(\"session_end\")\n)\n</code></pre>"},{"location":"pyspark/#4-identify-top-n-products-per-category-by-revenue","title":"4. Identify Top N Products Per Category by Revenue","text":"<p>Problem: Given a dataset of product sales, calculate revenue per product and return top 3 products in each category.</p> <p>Input Schema: <code>product_id</code>, <code>category</code>, <code>unit_price</code>, <code>quantity</code></p> <p>Expected Output: <code>category</code>, <code>product_id</code>, <code>revenue</code></p> <pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf = df.withColumn(\"revenue\", F.col(\"unit_price\") * F.col(\"quantity\"))\nwindow_spec = Window.partitionBy(\"category\").orderBy(F.desc(\"revenue\"))\ndf = df.withColumn(\"rank\", F.row_number().over(window_spec))\ntop_n_products = df.filter(F.col(\"rank\") &lt;= 3).select(\"category\", \"product_id\", \"revenue\")\n</code></pre> <p>Note: You can also use <code>dense_rank()</code> or <code>rank()</code> depending on how you want to treat ties in revenue.</p>"},{"location":"python/","title":"Python","text":"<p>title: Python Interview Questions description: A curated list of Python coding questions including medium-level problems and string manipulation challenges, commonly asked in data engineering and software interviews. keywords: Python, interview questions, coding problems, string algorithms, LeetCode, Python strings</p>"},{"location":"python/#python-interview-questions","title":"Python Interview Questions","text":""},{"location":"python/#medium-complexity-problems","title":"Medium Complexity Problems","text":""},{"location":"python/#1-longest-substring-without-repeating-characters","title":"1. Longest Substring Without Repeating Characters","text":"<p>Problem: Given a string, find the length of the longest substring without repeating characters. Approach: Use a sliding window and a hash set to keep track of characters in the current window. Code: <pre><code>def length_of_longest_substring(s: str) -&gt; int:\n    char_set = set()\n    left = 0\n    max_len = 0\n\n    for right in range(len(s)):\n        while s[right] in char_set:\n            char_set.remove(s[left])\n            left += 1\n        char_set.add(s[right])\n        max_len = max(max_len, right - left + 1)\n    return max_len\n</code></pre></p>"},{"location":"python/#2-3sum","title":"2. 3Sum","text":"<p>Problem: Given an array <code>nums</code> of <code>n</code> integers, find all unique triplets in the array which gives the sum of zero. Approach: Sort the array and use a two-pointer approach inside a loop. Code: <pre><code>def three_sum(nums: list[int]) -&gt; list[list[int]]:\n    nums.sort()\n    res = []\n    for i in range(len(nums) - 2):\n        if i &gt; 0 and nums[i] == nums[i - 1]:\n            continue\n        left, right = i + 1, len(nums) - 1\n        while left &lt; right:\n            s = nums[i] + nums[left] + nums[right]\n            if s &lt; 0:\n                left += 1\n            elif s &gt; 0:\n                right -= 1\n            else:\n                res.append([nums[i], nums[left], nums[right]])\n                while left &lt; right and nums[left] == nums[left + 1]:\n                    left += 1\n                while left &lt; right and nums[right] == nums[right - 1]:\n                    right -= 1\n                left += 1\n                right -= 1\n    return res\n</code></pre></p>"},{"location":"python/#string-related-questions","title":"String-Related Questions","text":""},{"location":"python/#1-reverse-words-in-a-string","title":"1. Reverse Words in a String","text":"<p>Problem: Given an input string <code>s</code>, reverse the order of the words. Code: <pre><code>def reverse_words(s: str) -&gt; str:\n    return \" \".join(reversed(s.strip().split()))\n</code></pre></p>"},{"location":"python/#2-valid-anagram","title":"2. Valid Anagram","text":"<p>Problem: Given two strings <code>s</code> and <code>t</code>, return true if <code>t</code> is an anagram of <code>s</code>, and false otherwise. Code: <pre><code>def is_anagram(s: str, t: str) -&gt; bool:\n    return sorted(s) == sorted(t)\n</code></pre></p>"},{"location":"sql/","title":"SQL Interview Questions","text":""},{"location":"sql/#q1-what-is-a-left-anti-join-how-is-it-different-from-left-join","title":"Q1. What is a LEFT ANTI JOIN? How is it different from LEFT JOIN?","text":"<p>A LEFT ANTI JOIN returns only the rows from the left table that do not have a match in the right table.</p> <p>This is different from a LEFT JOIN, which returns all rows from the left table and matching rows from the right (NULL if no match).</p>"},{"location":"sql/#example","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT *\nFROM orders o\nLEFT ANTI JOIN customers c\nON o.customer_id = c.id;\n</code></pre> <p>This query helps identify orphaned records \u2014 such as orders that are not linked to any customer.</p>"},{"location":"sql/#q2-write-a-sql-query-to-find-the-second-highest-salary-from-an-employees-table","title":"Q2. Write a SQL query to find the second highest salary from an <code>employees</code> table.","text":""},{"location":"sql/#example_1","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT MAX(salary) AS second_highest\nFROM employees\nWHERE salary &amp;lt; (\n  SELECT MAX(salary) FROM employees\n);\n</code></pre> <p>This uses a subquery to exclude the highest salary and returns the next highest.</p>"},{"location":"sql/#q3-what-is-the-difference-between-where-and-having-in-sql","title":"Q3. What is the difference between <code>WHERE</code> and <code>HAVING</code> in SQL?","text":"<ul> <li><code>WHERE</code> filters rows before aggregation.</li> <li><code>HAVING</code> filters after aggregation.</li> </ul>"},{"location":"sql/#example_2","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT department, COUNT(*) AS emp_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) &amp;gt; 5;\n</code></pre> <p>This query filters departments with more than 5 employees \u2014 <code>HAVING</code> is used after the <code>GROUP BY</code>.</p>"},{"location":"sql/#q4-write-a-query-to-find-duplicate-records-based-on-email-in-a-users-table","title":"Q4. Write a query to find duplicate records based on <code>email</code> in a <code>users</code> table.","text":""},{"location":"sql/#example_3","title":"\ud83d\udd0d Example:","text":"<pre><code>SELECT email, COUNT(*) AS count\nFROM users\nGROUP BY email\nHAVING COUNT(*) &amp;gt; 1;\n</code></pre> <p>This helps identify duplicates that can be cleaned or deduplicated.</p>"},{"location":"system-design/","title":"System Design","text":""},{"location":"system-design/#1-how-do-you-design-a-scalable-real-time-analytics-platform","title":"1. How do you design a scalable real-time analytics platform?","text":"<p>A real-time analytics platform typically includes: - Data Ingestion: Kafka / Kinesis - Stream Processing: Apache Flink / Spark Streaming - Storage: Hudi / Iceberg / Druid / ClickHouse - Serving Layer: Presto / Athena / API with caching - Ensure horizontal scalability, exactly-once semantics, and low latency.</p>"},{"location":"system-design/#2-what-are-the-core-tradeoffs-in-batch-vs-stream-processing","title":"2. What are the core tradeoffs in batch vs stream processing?","text":"Aspect Batch Processing Stream Processing Latency High (minutes to hours) Low (seconds to milliseconds) Data Freshness Delayed insights Real-time insights Tools Spark, dbt, Airflow Flink, Kafka Streams, Beam Use Cases Reports, ETL Monitoring, anomaly detection"},{"location":"system-design/#3-how-do-you-design-a-data-lakehouse","title":"3. How do you design a data lakehouse?","text":"<p>Key components: - Storage: S3/GCS + table formats like Apache Iceberg or Delta Lake - Metadata Layer: Catalog (Glue, Hive, Unity Catalog) - Processing Engine: Spark / Dask / Trino - Features:   - ACID transactions   - Schema evolution   - Time travel - Ensure separation of storage and compute, and support for both BI + ML workloads.</p>"}]}